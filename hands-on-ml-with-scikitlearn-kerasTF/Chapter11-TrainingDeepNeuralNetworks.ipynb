{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#The-Vanishing/Exploding-Gradients-Problems\" data-toc-modified-id=\"The-Vanishing/Exploding-Gradients-Problems-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>The Vanishing/Exploding Gradients Problems</a></span><ul class=\"toc-item\"><li><span><a href=\"#Glorot-and-He-Initialization\" data-toc-modified-id=\"Glorot-and-He-Initialization-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Glorot and He Initialization</a></span></li><li><span><a href=\"#Nonsaturating-Activation-Functions\" data-toc-modified-id=\"Nonsaturating-Activation-Functions-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Nonsaturating Activation Functions</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href=\"#Gradient-Clipping\" data-toc-modified-id=\"Gradient-Clipping-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Gradient Clipping</a></span></li></ul></li><li><span><a href=\"#Reusing-Pretained-Layers\" data-toc-modified-id=\"Reusing-Pretained-Layers-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reusing Pretained Layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transfer-Learning-with-Keras\" data-toc-modified-id=\"Transfer-Learning-with-Keras-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Transfer Learning with Keras</a></span></li><li><span><a href=\"#Unsupervised-Learning\" data-toc-modified-id=\"Unsupervised-Learning-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Unsupervised Learning</a></span></li><li><span><a href=\"#Pretaining-on-an-Auxiliary-Task\" data-toc-modified-id=\"Pretaining-on-an-Auxiliary-Task-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Pretaining on an Auxiliary Task</a></span></li></ul></li><li><span><a href=\"#Fast-Optimizers\" data-toc-modified-id=\"Fast-Optimizers-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fast Optimizers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Rate-Scheduling\" data-toc-modified-id=\"Learning-Rate-Scheduling-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Learning Rate Scheduling</a></span></li></ul></li><li><span><a href=\"#Avoid-Overfitting-Through-regularization\" data-toc-modified-id=\"Avoid-Overfitting-Through-regularization-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Avoid Overfitting Through regularization</a></span><ul class=\"toc-item\"><li><span><a href=\"#$\\ell_1$-and-$\\ell_2$-Regularization\" data-toc-modified-id=\"$\\ell_1$-and-$\\ell_2$-Regularization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>$\\ell_1$ and $\\ell_2$ Regularization</a></span></li><li><span><a href=\"#Dropout\" data-toc-modified-id=\"Dropout-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Dropout</a></span></li><li><span><a href=\"#Monte-Carlo(MC)-Dropout\" data-toc-modified-id=\"Monte-Carlo(MC)-Dropout-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Monte Carlo(MC) Dropout</a></span></li><li><span><a href=\"#Max-Norm-Regularization\" data-toc-modified-id=\"Max-Norm-Regularization-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Max-Norm Regularization</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Golort: None, tanh, logistic, softmax\n",
    "- He: ReLU and variants\n",
    "- LeCunL SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretaining on an Auxiliary Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Momentum Optimiization\n",
    "- Nesterov Accelerated Gradient\n",
    "- AdaGrad\n",
    "- RMSProp\n",
    "- Adam and Nadam Optimization\n",
    "\n",
    "Variants of Adam:\n",
    "- AdamMax\n",
    "- Nadam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Power scheduling\n",
    "- Exponential scheduling\n",
    "- Piecewise constant scheduling\n",
    "- Performance scheduling\n",
    "- 1cycle scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# power\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4) # decay is reverse of s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponential\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s) # lr * 0.1^(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model.fit(X, y, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piecewise constant\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance scheduling\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use keras.optimizers.schedules to define schedule lr\n",
    "\n",
    "It's step-based, example: When fitting a Keras model, decay every 100000 steps with a base\n",
    "of 0.96:\n",
    "\n",
    "```python\n",
    "def decayed_learning_rate(step):\n",
    "  return initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate,\n",
    "                                                             decay_steps=100000,\n",
    "                                                             decay_rate=0.96,\n",
    "                                                             staircase=True)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Overfitting Through regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\ell_1$ and $\\ell_2$ Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularizer will be called at each step and compute the regularization loss.\n",
    "layer = keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "RegularizedDense = partial(keras.layers.Dense, activation='relu', kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dropout at 0x7f45943f4160>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dropout(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo(MC) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test, training=True) for _ in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or wrap the Dropout layer, instead simply setting training=True to all layers(maybe BN layer in models)\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super(MCDropout, self).call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f4594612dd8>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for Dense Layber\n",
    "keras.layers.Dense(100, activation='relu', kernel_initializer='he_normal', \n",
    "                   kernel_constraint=keras.constraints.max_norm(1.0, axis=0))\n",
    "# for Conv2D\n",
    "keras.layers.Conv2D(64, kernel_size=(3,3), strides=(1,1), data_format='channels_last', activation='relu', \n",
    "                    kernel_constraint=keras.constraints.max_norm(2, axis=[0, 1, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
