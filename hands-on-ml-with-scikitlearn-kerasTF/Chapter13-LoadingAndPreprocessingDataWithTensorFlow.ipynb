{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Data-API\" data-toc-modified-id=\"Data-Data-API-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data Data API</a></span><ul class=\"toc-item\"><li><span><a href=\"#Chaining-Transformations\" data-toc-modified-id=\"Chaining-Transformations-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Chaining Transformations</a></span></li><li><span><a href=\"#Shuffling-the-Data\" data-toc-modified-id=\"Shuffling-the-Data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Shuffling the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Interleaving-lines-from-multiple-files\" data-toc-modified-id=\"Interleaving-lines-from-multiple-files-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Interleaving lines from multiple files</a></span></li></ul></li><li><span><a href=\"#Preprocessing-the-Data\" data-toc-modified-id=\"Preprocessing-the-Data-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Preprocessing the Data</a></span></li><li><span><a href=\"#Putting-Everything-Together\" data-toc-modified-id=\"Putting-Everything-Together-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Putting Everything Together</a></span></li><li><span><a href=\"#Prefetching\" data-toc-modified-id=\"Prefetching-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Prefetching</a></span></li><li><span><a href=\"#Using-the-Dataset-with-tf.keras\" data-toc-modified-id=\"Using-the-Dataset-with-tf.keras-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Using the Dataset with tf.keras</a></span></li></ul></li><li><span><a href=\"#The-TFRecord-Format\" data-toc-modified-id=\"The-TFRecord-Format-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>The TFRecord Format</a></span><ul class=\"toc-item\"><li><span><a href=\"#Compressed-TFRecord-Files\" data-toc-modified-id=\"Compressed-TFRecord-Files-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Compressed TFRecord Files</a></span></li><li><span><a href=\"#A-Brief-Introduction-to-Protocol-Buffers-(-protobufs-)\" data-toc-modified-id=\"A-Brief-Introduction-to-Protocol-Buffers-(-protobufs-)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>A Brief Introduction to Protocol Buffers ( <em>protobufs</em> )</a></span></li><li><span><a href=\"#TensorFlow-Protobufs\" data-toc-modified-id=\"TensorFlow-Protobufs-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>TensorFlow Protobufs</a></span></li><li><span><a href=\"#Loading-and-Parsing-Examples\" data-toc-modified-id=\"Loading-and-Parsing-Examples-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Loading and Parsing Examples</a></span></li><li><span><a href=\"#Handling-Lists-of-Lists-Using-the-SequenceExample-Protobuf\" data-toc-modified-id=\"Handling-Lists-of-Lists-Using-the-SequenceExample-Protobuf-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Handling Lists of Lists Using the SequenceExample Protobuf</a></span></li></ul></li><li><span><a href=\"#Preprocessing-the-Input-Features\" data-toc-modified-id=\"Preprocessing-the-Input-Features-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preprocessing the Input Features</a></span><ul class=\"toc-item\"><li><span><a href=\"#Encoding-Categorical-Features-Using-One-Hot-Vectors\" data-toc-modified-id=\"Encoding-Categorical-Features-Using-One-Hot-Vectors-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Encoding Categorical Features Using One-Hot Vectors</a></span></li><li><span><a href=\"#Encoding-Categorical-Features-Using-Embeddings\" data-toc-modified-id=\"Encoding-Categorical-Features-Using-Embeddings-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Encoding Categorical Features Using Embeddings</a></span></li><li><span><a href=\"#Keras-Preprocessing-Layers\" data-toc-modified-id=\"Keras-Preprocessing-Layers-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Keras Preprocessing Layers</a></span></li></ul></li><li><span><a href=\"#TF-Transform\" data-toc-modified-id=\"TF-Transform-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>TF Transform</a></span></li><li><span><a href=\"#The-TensorFlow-Datasets(TFDS)-Project\" data-toc-modified-id=\"The-TensorFlow-Datasets(TFDS)-Project-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>The TensorFlow Datasets(TFDS) Project</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Data API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (), types: tf.int32>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for data in dataset:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: tf.cast(x, dtype=tf.float32)**0.5, num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 0.  1.  4.  9. 16. 25. 36.], shape=(7,), dtype=float32)\n",
      "tf.Tensor([49. 64. 81.  0.  1.  4.  9.], shape=(7,), dtype=float32)\n",
      "tf.Tensor([16. 25. 36. 49. 64. 81.  0.], shape=(7,), dtype=float32)\n",
      "tf.Tensor([ 1.  4.  9. 16. 25. 36. 49.], shape=(7,), dtype=float32)\n",
      "tf.Tensor([64. 81.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = dataset.unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FilterDataset shapes: (), types: tf.float32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.filter(lambda x: x < 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(3):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.range(10).repeat(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# shufful only take elements from buffer\n",
    "# and buffer will filled by next first element from dataset\n",
    "# so when we set buffer_size=1, the shuffle make no sense\n",
    "dataset = dataset.shuffle(buffer_size=1, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2 0 3 5 4 6], shape=(7,), dtype=int64)\n",
      "tf.Tensor([7 8 0 9 1 3 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 2 7 6 9 0 8], shape=(7,), dtype=int64)\n",
      "tf.Tensor([2 1 4 3 5 6 7], shape=(7,), dtype=int64)\n",
      "tf.Tensor([9 8], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# if we change buffer_size=2\n",
    "# every time the dataset object will pick one element from buffer,\n",
    "# and place a new element from dataset on it immediately\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=2, seed=42).batch(7)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in the scenario above (with buffer=2):  \n",
    "0, 1 -> pick 1  \n",
    "0, 2 -> pick 2  \n",
    "0, 3 -> pick 0  \n",
    "4, 3 -> pick 3  \n",
    "4, 5 -> pick 5  \n",
    "4, 6 -> pick 4  \n",
    "7, 6 -> pick 6  \n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(\"/tf/notebooks/data/*.csv\", shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 2, 3, 4, 4, 5, 5, 4, 4, 5, 5, 4, 5, "
     ]
    }
   ],
   "source": [
    "# explaination of interleave():\n",
    "# 假定我们现在有一个Dataset--A\n",
    "# 从该A中取出cycle_length个element，然后对这些element apply map_func,得到cycle_length个新的Dataset对象。\n",
    "# 然后从这些新生成的Dataset对象中取数据，取数逻辑为轮流从每个对象里面取数据，每次取block_length个数据\n",
    "# 当这些新生成的某个Dataset的对象取尽时，从原Dataset中再取cycle_length个element，，然后apply map_func，以此类推。\n",
    "\n",
    "a = tf.data.Dataset.range(1, 6)  # ==> [ 1, 2, 3, 4, 5 ]\n",
    "b = a.interleave(lambda x: tf.data.Dataset.from_tensors(x).repeat(5),\n",
    "            cycle_length=3, block_length=2) \n",
    "for item in b:\n",
    "    print(item.numpy(),end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preporcess(line):\n",
    "    defs = [0.] * 8 + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    return tf.stack(fields[:-1]), tf.stack(fields[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([4.000e+00, 1.000e+01, 1.200e+01, 2.500e+01, 5.523e+03, 1.230e+02,\n",
       "        2.130e+02, 0.000e+00], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([10.], dtype=float32)>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preporcess(b'4, 10, 12, 25, 5523, 123, 213,, 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Everything Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, \n",
    "                       n_read_threads=None, shuffle_buffer_size=10000, \n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths, shuffle=True, seed=42)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepaths).skip(1), \n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preporcess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(buffer_size=shuffle_buffer_size).repeat(repeat)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Dataset with tf.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths)\n",
    "valid_set = csv_reader_dataset(valid_filepaths)\n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(0.05), input_shape=[30])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.mse, optimizer=keras.optimizers.Adam(0.005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_set, epochs=10, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or we use custom training loop\n",
    "for X_batch, y_batch in train_set:\n",
    "    [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other way is to create tf.function function to train\n",
    "@tf.function\n",
    "def train(model, optimizer, loss_fn, n_epochs, **kwargs):\n",
    "    train_set = csv_reader_dataset(train_filepath, repeat=n_epochs, **kwargs)\n",
    "    for X_batch, y_batch in train_set:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"This is the second record\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'This is the second record', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed TFRecord Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"Test compression funcionality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset('my_compressed.tfrecord', compression_type='GZIP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Test compression funcionality', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Introduction to Protocol Buffers ( _protobufs_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow Protobufs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializePartialToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Parsing Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serialized_example in tf.data.TFRecordDataset(['my_contacts.tfrecord']):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x7f914c195588>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch parsing\n",
    "for serialized_example in tf.data.TFRecordDataset(['my_contacts.tfrecord']).repeat(20).batch(10):\n",
    "    parsed_example = tf.io.parse_example(serialized_example, feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=string, numpy=\n",
       "array([b'Alice', b'Alice', b'Alice', b'Alice', b'Alice', b'Alice',\n",
       "       b'Alice', b'Alice', b'Alice', b'Alice'], dtype=object)>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_example['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Lists of Lists Using the SequenceExample Protobuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Input Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Lambda layer to customize standard deviation preprocess\n",
    "means = np.mean(X_train, axis=0, keepdims=True)\n",
    "stds = np.sstd(X_train, axis=0, keepdims=True)\n",
    "eps = keras.backend.epsilon()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)),\n",
    "    [...]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-contained layer\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def apdat(self, data_sample):\n",
    "        sefl.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_layer = Standardization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using One-Hot Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['<1H OCEAN', 'INLAND', \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "num_oov_buckets = 2\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 3, 5, 1, 1])>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the loopup table\n",
    "categories = tf.constant([\"<1H OCEAN\", 'NEAR BAY', \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 7), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n",
    "cat_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorical Features Using Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(7, 2) dtype=float32, numpy=\n",
       "array([[0.16258681, 0.07541919],\n",
       "       [0.8378128 , 0.8445947 ],\n",
       "       [0.15343809, 0.19920743],\n",
       "       [0.6615534 , 0.21533203],\n",
       "       [0.6658559 , 0.28871882],\n",
       "       [0.181041  , 0.29205155],\n",
       "       [0.7755804 , 0.11226654]], dtype=float32)>"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 2\n",
    "embed_init = tf.random.uniform([len(vocab)+num_oov_buckets, embedding_dim])\n",
    "embedding_matrix = tf.Variable(embed_init)\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = tf.constant(['NEAR BAY', \"DESERT\", \"INLAND\", \"INLAND\"])\n",
    "cat_indices = table.lookup(categories)\n",
    "cat_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[0.6615534 , 0.21533203],\n",
       "       [0.181041  , 0.29205155],\n",
       "       [0.8378128 , 0.8445947 ],\n",
       "       [0.8378128 , 0.8445947 ]], dtype=float32)>"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.embedding_lookup(embedding_matrix, cat_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2), dtype=float32, numpy=\n",
       "array([[ 0.01041894,  0.02498868],\n",
       "       [ 0.00501535, -0.00349748],\n",
       "       [-0.0422516 ,  0.04752635],\n",
       "       [-0.0422516 ,  0.04752635]], dtype=float32)>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use keras\n",
    "embedding = keras.layers.Embedding(input_dim=len(vocab)+num_oov_buckets, output_dim=embedding_dim)\n",
    "embedding(cat_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model take regular inputs(numeric) and categorical inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regualr inputs and categorical inputs\n",
    "regular_inputs = keras.layers.Input(shape=[8,])\n",
    "categories = keras.layers.Input([], dtype=tf.string)\n",
    "# index & embedding layers\n",
    "cat_indices = keras.layers.Lambda(lambda cate: table.lookup(cate))(categories)\n",
    "cat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\n",
    "# concatenate two inputs\n",
    "encoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\n",
    "# output\n",
    "outputs = keras.layers.Dense(1)(encoded_inputs)\n",
    "# model\n",
    "model = keras.models.Model(inputs=[regular_inputs, categories], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (None,)              0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 8)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 2)            12          lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10)           0           input_1[0][0]                    \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            11          concatenate[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 23\n",
      "Trainable params: 23\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Preprocessing Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets(TFDS) Project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
