{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.runners import DirectRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCollections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be: that is the question: \n",
      "Whether 'tis nobler in the mind to suffer \n",
      "The slings and arrows of outrageous fortune, \n",
      "Or to take arms against a sea of troubles, \n"
     ]
    }
   ],
   "source": [
    "## from file\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline | 'ReadMyFile' >> beam.io.ReadFromText(\n",
    "    'gs://some/inputData.txt')\n",
    "    \n",
    "## from in-memory data\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([\n",
    "        'To be, or not to be: that is the question: ',\n",
    "        \"Whether 'tis nobler in the mind to suffer \",\n",
    "        'The slings and arrows of outrageous fortune, ',\n",
    "        'Or to take arms against a sea of troubles, ',\n",
    "      ]) \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ParDo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParDo with DoFn\n",
      "43\n",
      "42\n",
      "45\n",
      "43\n",
      "ParDo with lambda\n",
      "43\n",
      "42\n",
      "45\n",
      "43\n",
      "replace ParDo with Map\n",
      "43\n",
      "42\n",
      "45\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "# NOTE: ParDo takes a function which returns \n",
    "# 0/more/iterable(e.g. instead return, use yield) elements\n",
    "\n",
    "## ParDo with DoFn\n",
    "print('ParDo with DoFn')\n",
    "class ComputeWordLengthFn(beam.DoFn):\n",
    "    # implement `process` function\n",
    "    def process(self, element):\n",
    "        return [len(element)]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([\n",
    "        'To be, or not to be: that is the question: ',\n",
    "        \"Whether 'tis nobler in the mind to suffer \",\n",
    "        'The slings and arrows of outrageous fortune, ',\n",
    "        'Or to take arms against a sea of troubles, ',\n",
    "      ]) \\\n",
    "    | 'ApplyParDo' >> beam.ParDo(ComputeWordLengthFn()) \\\n",
    "    | 'output' >> beam.Map(print)\n",
    "\n",
    "## ParDo with lambda\n",
    "print('ParDo with lambda')\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([\n",
    "        'To be, or not to be: that is the question: ',\n",
    "        \"Whether 'tis nobler in the mind to suffer \",\n",
    "        'The slings and arrows of outrageous fortune, ',\n",
    "        'Or to take arms against a sea of troubles, ',\n",
    "      ]) \\\n",
    "    | 'ApplyParDo' >> beam.ParDo(lambda e: [len(e)]) \\\n",
    "    | 'output' >> beam.Map(print)\n",
    "    \n",
    "## replace ParDo with Map\n",
    "## if only output ONE element\n",
    "print('replace ParDo with Map')\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([\n",
    "        'To be, or not to be: that is the question: ',\n",
    "        \"Whether 'tis nobler in the mind to suffer \",\n",
    "        'The slings and arrows of outrageous fortune, ',\n",
    "        'Or to take arms against a sea of troubles, ',\n",
    "      ]) \\\n",
    "    | 'ApplyMap' >> beam.Map(len) \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', [1, 9])\n",
      "('b', [2, 15])\n",
      "('c', [10])\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([\n",
    "        ('a', 1),\n",
    "        ('b', 2),\n",
    "        ('c', 10),\n",
    "        ('b', 15),\n",
    "        ('a', 9)\n",
    "      ]) \\\n",
    "    | 'GroupByKey' >> beam.GroupByKey() \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoGroupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('amy', {'emails': ['amy@example.com'], 'phones': ['111-222-3333', '333-444-5555']})\n",
      "('james', {'emails': [], 'phones': ['222-333-4444']})\n",
      "('carl', {'emails': ['carl@example.com', 'carl@email.com'], 'phones': ['444-555-6666']})\n",
      "('julia', {'emails': ['julia@example.com'], 'phones': []})\n"
     ]
    }
   ],
   "source": [
    "emails_list = [\n",
    "    ('amy', 'amy@example.com'),\n",
    "    ('carl', 'carl@example.com'),\n",
    "    ('julia', 'julia@example.com'),\n",
    "    ('carl', 'carl@email.com'),\n",
    "]\n",
    "phones_list = [\n",
    "    ('amy', '111-222-3333'),\n",
    "    ('james', '222-333-4444'),\n",
    "    ('amy', '333-444-5555'),\n",
    "    ('carl', '444-555-6666'),\n",
    "]\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    emails = pipeline | 'CreateEmails' >> beam.Create(emails_list)\n",
    "    phones = pipeline | 'CreatePhones' >> beam.Create(phones_list)\n",
    "    emails_phones = {'emails': emails, 'phones': phones}\n",
    "    results = (emails_phones | beam.CoGroupByKey())\n",
    "    results | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CombineGlobally takes all elements\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([1,2,3,4,5,6,7,8,9]) \\\n",
    "    | 'CombineGlobally' >> beam.CombineGlobally(sum) \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CombineFn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n"
     ]
    }
   ],
   "source": [
    "class AverageFn(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return (0.0, 0)\n",
    "\n",
    "    def add_input(self, sum_count, input):\n",
    "        (sum, count) = sum_count\n",
    "        return sum + input, count + 1\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        sums, counts = zip(*accumulators)\n",
    "        return sum(sums), sum(counts)\n",
    "\n",
    "    def extract_output(self, sum_count):\n",
    "        (sum, count) = sum_count\n",
    "        return sum / count if count else float('NaN')\n",
    "\n",
    "## NOTE: To have Combine instead return an empty PCollection if the input is empty,\n",
    "## specify .withoutDefaults when you apply your Combine transform\n",
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([1,2,3,4,5,6,7,8,9]) \\\n",
    "    | 'CustomizedCombineFn' >> beam.CombineGlobally(AverageFn()).without_defaults() \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CombinePerKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([1,2,3,4,5,6,7,8,9]) \\\n",
    "    | 'CombinePerKey' >> beam.CombinePerKey(beam.combiners.MeanCombineFn()) \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "12\n",
      "91\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    numbers1 = pipeline | 'CreateNumbers1' >> beam.Create([1,2,3,4,5])\n",
    "    numbers2 = pipeline | 'CreateNumbers2' >> beam.Create([12,91,100])\n",
    "    (numbers1, numbers2) | 'Flatten' >> beam.Flatten() \\\n",
    "    | 'output' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "perennial: {'icon': 'ðŸ“', 'name': 'Strawberry', 'duration': 'perennial'}\n",
      "1\n",
      "biennial: {'icon': 'ðŸ¥•', 'name': 'Carrot', 'duration': 'biennial'}\n",
      "2\n",
      "perennial: {'icon': 'ðŸ†', 'name': 'Eggplant', 'duration': 'perennial'}\n",
      "0\n",
      "annual: {'icon': 'ðŸ…', 'name': 'Tomato', 'duration': 'annual'}\n",
      "2\n",
      "perennial: {'icon': 'ðŸ¥”', 'name': 'Potato', 'duration': 'perennial'}\n"
     ]
    }
   ],
   "source": [
    "## Partition is a Beam transform for PCollection objects that store the same data type. \n",
    "## Partition splits a single PCollection into a fixed number of smaller collections.\n",
    "\n",
    "## NOTE: Partition takes in a function which returns a index/number \n",
    "## to identify which partition the element should belongs to\n",
    "\n",
    "durations = ['annual', 'biennial', 'perennial']\n",
    "\n",
    "def by_duration(plant, num_partitions):\n",
    "#     print(durations.index(plant['duration']))\n",
    "    return durations.index(plant['duration'])\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "    annuals, biennials, perennials = (\n",
    "        pipeline \\\n",
    "        | 'Gardening plants' >> beam.Create([\n",
    "            {'icon': 'ðŸ“', 'name': 'Strawberry', 'duration': 'perennial'},\n",
    "            {'icon': 'ðŸ¥•', 'name': 'Carrot', 'duration': 'biennial'},\n",
    "            {'icon': 'ðŸ†', 'name': 'Eggplant', 'duration': 'perennial'},\n",
    "            {'icon': 'ðŸ…', 'name': 'Tomato', 'duration': 'annual'},\n",
    "            {'icon': 'ðŸ¥”', 'name': 'Potato', 'duration': 'perennial'},\n",
    "        ]) \\\n",
    "        | 'Partition' >> beam.Partition(by_duration, len(durations))\n",
    "    )\n",
    "\n",
    "    annuals | 'Annuals' >> beam.Map(lambda x: print('annual: {}'.format(x)))\n",
    "    biennials | 'Biennials' >> beam.Map(lambda x: print('biennial: {}'.format(x)))\n",
    "    perennials | 'Perennials' >> beam.Map(lambda x: print('perennial: {}'.format(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether 'tis nobler in the mind to suffer \n",
      "42\n",
      "The slings and arrows of outrageous fortune, \n"
     ]
    }
   ],
   "source": [
    "# Side inputs are available as extra arguments in the DoFn's process method or Map / FlatMap's callable.\n",
    "# Optional, positional, and keyword arguments are all supported. Deferred arguments are unwrapped into their\n",
    "# actual values. For example, using pvalue.AsIteor(pcoll) at pipeline construction time results in an iterable\n",
    "# of the actual elements of pcoll being passed into each process invocation. In this example, side inputs are\n",
    "# passed to a FlatMap transform as extra arguments and consumed by filter_using_length.\n",
    "\n",
    "def filter_using_length(word, lower_bound, upper_bound=float('inf')):\n",
    "    if lower_bound <= len(word) <= upper_bound:\n",
    "        yield word\n",
    "\n",
    "class FilterUsingLength(beam.DoFn):\n",
    "    def process(self, element, lower_bound, upper_bound=float('inf')):\n",
    "        if lower_bound <= len(element) <= upper_bound:\n",
    "            # use yield to produce iterable returns\n",
    "            yield len(element)\n",
    "\n",
    "with beam.Pipeline() as pipeline:\n",
    "\n",
    "    # construct a deferred side input\n",
    "    words = pipeline \\\n",
    "    | 'CreateFromMemory' >> beam.Create([\n",
    "        'To be, or not to be: that is the question: ',\n",
    "        \"Whether 'tis nobler in the mind to suffer \",\n",
    "        'The slings and arrows of outrageous fortune, ',\n",
    "        'Or to take arms against a sea of troubles, ',\n",
    "      ])\n",
    "    avg_len = words \\\n",
    "    | beam.Map(len) \\\n",
    "    | beam.CombineGlobally(beam.combiners.MeanCombineFn())\n",
    "\n",
    "    # call with explicit side inputs in FlatMap\n",
    "    small_words = (\n",
    "        words \\\n",
    "        | 'small' >> beam.FlatMap(filter_using_length, lower_bound=40, upper_bound=42) \\\n",
    "        | 'p1' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    # call with deferred side input in FlatMap\n",
    "    larger_than_avg = (\n",
    "        words \\\n",
    "        | 'larger' >> beam.FlatMap(filter_using_length, lower_bound=beam.pvalue.AsSingleton(avg_len)) \\\n",
    "        | 'p2' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    # call with explicit side inputs in ParDo\n",
    "    small_words_2 = (\n",
    "        words \\\n",
    "        | 'small2' >> beam.ParDo(FilterUsingLength(), 40, 42) \\\n",
    "        | 'p3' >> beam.Map(print)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n",
      "c\n",
      "x5\n",
      "3\n",
      "x5\n"
     ]
    }
   ],
   "source": [
    "from apache_beam import pvalue\n",
    "\n",
    "## use pvalue.TaggedOutput to define the name of specific output\n",
    "## then the name can be used in with_outputs() method\n",
    "\n",
    "class ProcessWords(beam.DoFn):\n",
    "    def process(self, element, cutoff_length, marker):\n",
    "        if len(element) <= cutoff_length:\n",
    "            # Emit this short word to the main output.\n",
    "            yield element\n",
    "        else:\n",
    "            # Emit this word's long length to the 'above_cutoff_lengths' output.\n",
    "            yield pvalue.TaggedOutput('above_cutoff_lengths', len(element))\n",
    "        if element.startswith(marker):\n",
    "            # Emit this word to a different output with the 'marked strings' tag.\n",
    "            yield pvalue.TaggedOutput('marked strings', element)\n",
    "\n",
    "words = ['aa', 'bbb', 'c', 'x5']\n",
    "with beam.Pipeline() as pipeline:\n",
    "    results = (\n",
    "        words \\\n",
    "        | beam.ParDo(ProcessWords(), cutoff_length=2, marker='x').with_outputs(\n",
    "            'above_cutoff_lengths',\n",
    "            'marked strings',\n",
    "            main='below_cutoff_strings')\n",
    "    )\n",
    "    below = results.below_cutoff_strings | 'p1' >> beam.Map(print)\n",
    "    above = results.above_cutoff_lengths | 'p2' >> beam.Map(print)\n",
    "    marked = results['marked strings'] | 'p3' >> beam.Map(print) # indexing works as well\n",
    "    \n",
    "    ## unzipping also works\n",
    "    below, above, marked = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1\n",
      "3\n",
      "11\n",
      "2\n",
      "4\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "## Emitting to multiple outputs in your DoFn\n",
    "# NOTE: if with_outputs() `main` argument is not assigned, then the main output is `None` - results[None]\n",
    "def even_odd(x):\n",
    "    yield pvalue.TaggedOutput('odd' if x % 2 else 'even', x)\n",
    "    if x % 10 == 0:\n",
    "        yield x\n",
    "\n",
    "numbers = [1,2,3,4,11,10]\n",
    "with beam.Pipeline() as pipeline:\n",
    "    evens, odds, tens = (\n",
    "        numbers \\\n",
    "        | beam.ParDo(even_odd).with_outputs('odd', 'even', main='tens')\n",
    "    )\n",
    "    evens | 'p1' >> beam.Map(print)\n",
    "    odds | 'p2' >> beam.Map(print)\n",
    "    tens | 'p3' >> beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## access TimeStamp of an input element, add a keyword parameter default to DoFn.TimeStampParam\n",
    "class ProcessRecord(beam.DoFn):\n",
    "    def process(self, element, timestamp=beam.DoFn.TimestampParam):\n",
    "        # access timestamp of element.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To access the window an input element falls into, add a keyword parameter default to DoFn.WindowParam\n",
    "class ProcessRecord(beam.DoFn):\n",
    "    def process(self, element, window=beam.DoFn.WindowParam):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## When triggers are used, Beam provides a DoFn.PaneInfoParam object that contains information about the current firing.\n",
    "## Using DoFn.PaneInfoParam you can determine whether this is an early or a late firing, \n",
    "## and how many times this window has already fired for this key\n",
    "class ProcessRecord(beam.DoFn):\n",
    "    def process(self, element, pane_info=beam.DoFn.PaneInfoParam):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "7\n",
      "2\n",
      "1\n",
      "[('x5', 1), ('c', 2), ('bbb', 1)]\n"
     ]
    }
   ],
   "source": [
    "## subclass PTransform to create composite transforms\n",
    "## NOTE: Within your PTransform subclass, youâ€™ll need to override the expand method. \n",
    "## The expand method is where you add the processing logic for the PTransform\n",
    "\n",
    "words = ['aa', 'bbb', 'c', 'x5', 'Test', 'awesome', 'aa', 'c']\n",
    "\n",
    "class ComputeWordLength(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll \\\n",
    "            | beam.Map(lambda x: len(x)) \\\n",
    "            | beam.Map(print)\n",
    "        )\n",
    "\n",
    "class CountWords(beam.PTransform):\n",
    "    def expand(self, pcoll):\n",
    "        return (\n",
    "            pcoll \\\n",
    "            | beam.combiners.Count.PerElement() \\\n",
    "            | beam.combiners.Top.Of(3) \\\n",
    "            | beam.Map(print)\n",
    "        )\n",
    "\n",
    "# use ComputeWordLength composite PTransform\n",
    "with beam.Pipeline() as pipeline:\n",
    "    words | ComputeWordLength()\n",
    "with beam.Pipeline() as pipeline:\n",
    "    words | CountWords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
