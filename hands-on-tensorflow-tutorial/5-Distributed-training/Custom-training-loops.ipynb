{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.expand_dims(train_images, axis=-1)\n",
    "test_images = tf.expand_dims(test_images, axis=-1)\n",
    "\n",
    "train_images = tf.cast(train_images, tf.float32) / 255.0\n",
    "test_images = tf.cast(test_images, tf.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a strategy to distribute the variables and the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does `tf.distribute.MirroredStrategy` strategy work?\n",
    "\n",
    "- All the variables and the model graph is replicated on the replicas.\n",
    "- Input is evenly distributed across the replicas.\n",
    "- Each replica calculates the loss and gradients for the input it received.\n",
    "- The gradients are synced across all the replicas by summing them.\n",
    "- After the sync, the same update is made to the copies of the variables on each replica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_images)\n",
    "\n",
    "BATCH_SIZE_PER_REPLICA = 64\n",
    "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(BATCH_SIZE)\n",
    "\n",
    "# Distribute dataset\n",
    "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dist_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definee the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, on a single machine with 1 GPU/CPU, loss is divided by the number of examples in the batch of input.\n",
    "\n",
    "So, how should the loss be calculated when using a `tf.distribute.Strategy`?\n",
    "\n",
    "- For an example, let's say you have 4 GPU's and a batch size of 64. One batch of input is distributed across the replicas (4 GPUs), each replica getting an input of size 16.\n",
    "- The model on each replica does a forward pass with its respective input and calculates the loss. Now, instead of dividing the loss by the number of examples in its respective input (BATCH_SIZE_PER_REPLICA = 16), the loss should be divided by the GLOBAL_BATCH_SIZE (64).\n",
    "\n",
    "Why do this?\n",
    "\n",
    "- This needs to be done because after the gradients are calculated on each replica, they are synced across the replicas by summing them.\n",
    "\n",
    "How to do this in TensorFlow?\n",
    "\n",
    "- If you're writing a custom training loop, as in this tutorial, you should sum the per example losses and divide the sum by the GLOBAL_BATCH_SIZE: scale_loss = tf.reduce_sum(loss) * (1. / GLOBAL_BATCH_SIZE) or you can use `tf.nn.compute_average_loss` which takes the per example loss, optional sample weights, and GLOBAL_BATCH_SIZE as arguments and returns the scaled loss.\n",
    "- If you are using regularization losses in your model then you need to scale the loss value by number of replicas. You can do this by using the `tf.nn.scale_regularization_loss` function.\n",
    "- Using `tf.reduce_mean` is __not recommended__. Doing so divides the loss by actual per replica batch size which may vary step to step.\n",
    "- This reduction and scaling is done automatically in keras `model.compile` and `model.fit`\n",
    "- If using `tf.keras.losses` classes (as in the example below), the loss reduction needs to be explicitly specified to be one of `NONE` or `SUM`. `AUTO` and `SUM_OVER_BATCH_SIZE` are disallowed when used with tf.distribute.Strategy. `AUTO` is disallowed because the user should explicitly think about what reduction they want to make sure it is correct in the distributed case. `SUM_OVER_BATCH_SIZE` is disallowed because currently it would only divide by per replica batch size, and leave the dividing by number of replicas to the user, which might be easy to miss. So instead we ask the user do the reduction themselves explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True,\n",
    "        reduction=tf.keras.losses.Reduction.NONE\n",
    "    )\n",
    "    \n",
    "    def compute_loss(labels, predictions):\n",
    "        per_example_loss = loss_object(labels, predictions)\n",
    "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE_PER_REPLICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the metrics to track loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuuracy')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inputs):\n",
    "    \"\"\"Returns per-replica loss (sum_replica_loss / (replica_batch_size * num_replicas))\"\"\"\n",
    "    images, labels = inputs\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images, training=True)\n",
    "        loss = compute_loss(labels, predictions) # replica loss (divided by num_replicas)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_accuracy.update_state(labels, predictions)\n",
    "    return loss\n",
    "\n",
    "def test_step(inputs):\n",
    "    images, labels = inputs\n",
    "    predictions = model(images, training=False)\n",
    "    loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss.update_state(loss)\n",
    "    test_accuracy.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch  1, Loss: 0.5006, Accuracy: 82.23%, Test Loss: 0.3768, Test Accuracy: 86.72%\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch  2, Loss: 0.3294, Accuracy: 88.15%, Test Loss: 0.3332, Test Accuracy: 88.02%\n",
      "Epoch  3, Loss: 0.2852, Accuracy: 89.57%, Test Loss: 0.3036, Test Accuracy: 89.11%\n",
      "Epoch  4, Loss: 0.2558, Accuracy: 90.68%, Test Loss: 0.2922, Test Accuracy: 89.06%\n",
      "Epoch  5, Loss: 0.2322, Accuracy: 91.50%, Test Loss: 0.2711, Test Accuracy: 90.02%\n",
      "Epoch  6, Loss: 0.2132, Accuracy: 92.06%, Test Loss: 0.2708, Test Accuracy: 89.97%\n",
      "Epoch  7, Loss: 0.1953, Accuracy: 92.82%, Test Loss: 0.2728, Test Accuracy: 90.05%\n",
      "Epoch  8, Loss: 0.1789, Accuracy: 93.43%, Test Loss: 0.2789, Test Accuracy: 90.00%\n",
      "Epoch  9, Loss: 0.1652, Accuracy: 93.92%, Test Loss: 0.2718, Test Accuracy: 90.59%\n",
      "Epoch 10, Loss: 0.1508, Accuracy: 94.40%, Test Loss: 0.2680, Test Accuracy: 90.79%\n"
     ]
    }
   ],
   "source": [
    "# 'run' replicates the provided computation and runs it with the distributed input.\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(dataset_inputs):\n",
    "    \"\"\"Return average loss across all replicas\"\"\"\n",
    "    per_replica_losses = strategy.run(train_step, args=(dataset_inputs, ))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(dataset_inputs):\n",
    "    return strategy.run(test_step, args=(dataset_inputs, ))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # TRAIN LOOP\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in train_dist_dataset:\n",
    "        total_loss += distributed_train_step(x)\n",
    "        num_batches += 1\n",
    "    train_loss = total_loss / num_batches\n",
    "    \n",
    "    # TEST LOOP\n",
    "    for x in test_dist_dataset:\n",
    "        distributed_test_step(x)\n",
    "        \n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint.save('./training_checkpoints/ckpt')\n",
    "        \n",
    "    template = (\"Epoch {:2d}, Loss: {:.4f}, Accuracy: {:.2%}, Test Loss: {:.4f}, Test Accuracy: {:.2%}\")\n",
    "    print(template.format(epoch+1, train_loss, train_accuracy.result(), test_loss.result(), test_accuracy.result()))\n",
    "    \n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='eval_accuracy')\n",
    "\n",
    "new_model = create_model()\n",
    "new_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(images, labels):\n",
    "    predictions = new_model(images, training=False)\n",
    "    eval_accuracy.update_state(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after restoring the saved model without strategy: 91.04%\n"
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.Checkpoint(optimizer=new_optimizer, model=new_model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint('./training_checkpoints'))\n",
    "\n",
    "for images, labels in test_dataset:\n",
    "    eval_step(images, labels)\n",
    "    \n",
    "print('Accuracy after restoring the saved model without strategy: {:.2%}'.format(eval_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate ways of iterating over a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Loss: 0.1528, Accuracy: 95.16%\n",
      "Epoch  2, Loss: 0.1205, Accuracy: 95.47%\n",
      "Epoch  3, Loss: 0.1164, Accuracy: 95.63%\n",
      "Epoch  4, Loss: 0.0980, Accuracy: 96.25%\n",
      "Epoch  5, Loss: 0.1277, Accuracy: 95.47%\n",
      "Epoch  6, Loss: 0.1487, Accuracy: 94.38%\n",
      "Epoch  7, Loss: 0.1313, Accuracy: 95.31%\n",
      "Epoch  8, Loss: 0.1460, Accuracy: 94.69%\n",
      "Epoch  9, Loss: 0.1097, Accuracy: 96.72%\n",
      "Epoch 10, Loss: 0.0970, Accuracy: 96.09%\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    train_iter = iter(train_dist_dataset)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        total_loss += distributed_train_step(next(train_iter))\n",
    "        num_batches += 1\n",
    "    average_train_loss = total_loss / num_batches\n",
    "    \n",
    "    template = (\"Epoch {:2d}, Loss: {:.4f}, Accuracy: {:.2%}\")\n",
    "    print(template.format(epoch+1, average_train_loss, train_accuracy.result()))\n",
    "    train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate inside a tf.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0, Loss: 0.0736, Accuracy: 97.17%\n",
      "Epoch  1, Loss: 0.0679, Accuracy: 97.53%\n",
      "Epoch  2, Loss: 0.0605, Accuracy: 97.73%\n",
      "Epoch  3, Loss: 0.0570, Accuracy: 97.91%\n",
      "Epoch  4, Loss: 0.0542, Accuracy: 97.97%\n",
      "Epoch  5, Loss: 0.0453, Accuracy: 98.34%\n",
      "Epoch  6, Loss: 0.0450, Accuracy: 98.34%\n",
      "Epoch  7, Loss: 0.0398, Accuracy: 98.55%\n",
      "Epoch  8, Loss: 0.0409, Accuracy: 98.46%\n",
      "Epoch  9, Loss: 0.0331, Accuracy: 98.78%\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def distributed_train_epoch(dataset):\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for x in dataset:\n",
    "        per_replica_losses = strategy.run(train_step, args=(x, ))\n",
    "        total_loss += strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "        num_batches += 1\n",
    "    return total_loss / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss = distributed_train_epoch(train_dist_dataset)\n",
    "    \n",
    "    template = ('Epoch {:2d}, Loss: {:.4f}, Accuracy: {:.2%}')\n",
    "    print(template.format(epoch, train_loss, train_accuracy.result()))\n",
    "    train_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./training_checkpoints/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
