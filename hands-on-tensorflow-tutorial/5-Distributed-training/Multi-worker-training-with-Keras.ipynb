{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_dataset(batch_size):\n",
    "    (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "    x_train = x_train / 255.0\n",
    "    y_train = y_train.astype(np.int64)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=[28, 28]),\n",
    "        tf.keras.layers.Reshape(target_shape=[28, 28, 1]),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.2663 - accuracy: 0.1650\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.1825 - accuracy: 0.4480\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 2.0727 - accuracy: 0.6199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0932eac90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_worker_batch_size = 64\n",
    "single_worker_dataset = mnist_dataset(per_worker_batch_size)\n",
    "single_worker_model = create_model()\n",
    "single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-worker Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's enter the world of multi-worker training. In TensorFlow, `TF_CONFIG` environment variable is required for training on multiple machines, each of which possibly has a different role. `TF_CONFIG` is a JSON string used to specify the cluster configuration on each worker that is part of the cluster.\n",
    "\n",
    "There are two components of `TF_CONFIG`: `cluster` and `task`. cluster provides information about the training cluster, which is a dict consisting of different types of jobs such as `worker`. In multi-worker training, there is usually one `worker` that takes on a little more responsibility like saving checkpoint and writing summary file for TensorBoard in addition to what a regular worker does. Such worker is referred to as the _'chief'_ worker, and it is customary that the `worker` with `index` 0 is appointed as the chief `worker` (in fact this is how `tf.distribute.Strategy` is implemented). `task` on the other hand provides information of the current task. The first component `cluster` is the same for all workers, and the second component `task` is different on each worker  and specifies the `type` and `index` of that worker.\n",
    "\n",
    "In this example, we set the task `type` to `\"worker\"` and the task `index` to `0`. This means the machine that has such setting is the first worker, which will be appointed as the chief worker and do more work than other workers. Note that other machines will need to have `TF_CONFIG` environment variable set as well, and it should have the same `cluster` dict, but different task `type` or task `index` depending on what the roles of those machines are.\n",
    "\n",
    "For illustration purposes, this tutorial shows how one may set a `TF_CONFIG` with 2 workers on `localhost`. In practice, users would create multiple workers on external IP addresses/ports, and set `TF_CONFIG` on each worker appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps({\n",
    "    'cluster': {\n",
    "        'worker': ['localhost: 12345', 'localhost: 23456']\n",
    "    },\n",
    "    'task': {\n",
    "        'type': 'worker', \n",
    "        'index': 0\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0',)\n",
      "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CollectiveCommunication.AUTO\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model with MultiWorkerMirroredStrategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Always pass in `steps_per_epoch` argument to `model.fit()` since `MultiWorkerMirroredStrategy` does not support last partial batch handling. When using `steps_per_epoch`, `model.fit()` does not create a new iterator from the input every epoch, but continues from wherever the last epoch ended. Hence, make sure to call `.repeat()` on the dataset so it has an adequate number of examples for N epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - 0s 6ms/step - accuracy: 0.1225 - loss: 2.2884\n",
      "Epoch 2/3\n",
      "70/70 [==============================] - 0s 5ms/step - accuracy: 0.2888 - loss: 2.2439\n",
      "Epoch 3/3\n",
      "70/70 [==============================] - 0s 5ms/step - accuracy: 0.4684 - loss: 2.1890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcee87d2490>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_worker = 4\n",
    "\n",
    "# Here the batch size scales up by number of workers\n",
    "# since `tf.data.Dataset.batch` expects the global batch size,\n",
    "global_batch_size = per_worker_batch_size * num_worker\n",
    "multi_worker_dataset = mnist_dataset(global_batch_size)\n",
    "\n",
    "with strategy.scope():\n",
    "    multi_worker_model = create_model()\n",
    "    \n",
    "multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset sharding and batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multi-worker training, sharding data into multiple parts is needed to ensure convergence and performance. However, note that in above code snippet, the datasets are directly sent to `model.fit()` without needing to shard; this is because `tf.distribute.Strategy` API takes care of the dataset sharding automatically in multi-worker trainings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer manual sharding for four training, automatic sharding can be turned off via `tf.data.experimental.DistributeOption` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "dataset_no_auto_shard = multi_worker_dataset.with_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `MultiWorkerMirroredStrategy` provides multiple collective communication implementations. `RING` implements ring-based collectives using gRPC as the cross-host communication layer. `NCCL` uses Nvidia's NCCL to implement collectives. `AUTO` defers the choice to the runtime. The best choice of collective implementation depends upon the number and kind of GPUs, and the network interconnect in the cluster. To override the automatic choice, specify a valid value to the `communication` parameter of MultiWorkerMirroredStrategy's constructor, e.g. `communication=tf.distribute.experimental.CollectiveCommunication.NCCL`.\n",
    "- Cast the variables to `tf.float` if possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Keras` with `tf.distribute.Strategy` comes with the adcantage of fault tolerance in cases where workers dir or are otherwise unstable. We do this by preserving training state in the distributed file system of your choice, such that upon restart of the instance that previously fialed or preempted, the training state is recorvered.\n",
    "\n",
    "If a worker gets preempted, the whole cluster pauses until the preempted worker is restarted. Once the worker rejoins the cluster, other workers will also restart. Now, every worker reads the checkpoint file that was previously saved and picks up its former state, thereby allowing the cluster to get back in sync. Then the training continues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "70/70 [==============================] - ETA: 0s - accuracy: 0.1205 - loss: 2.2786WARNING:tensorflow:From /home/kaimo/.local/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./training/ckpt/assets\n",
      "70/70 [==============================] - 1s 13ms/step - accuracy: 0.1205 - loss: 2.2786\n",
      "Epoch 2/3\n",
      "63/70 [==========================>...] - ETA: 0s - accuracy: 0.3441 - loss: 2.2098INFO:tensorflow:Assets written to: ./training/ckpt/assets\n",
      "70/70 [==============================] - 1s 12ms/step - accuracy: 0.3504 - loss: 2.2061\n",
      "Epoch 3/3\n",
      "66/70 [===========================>..] - ETA: 0s - accuracy: 0.4626 - loss: 2.1261INFO:tensorflow:Assets written to: ./training/ckpt/assets\n",
      "70/70 [==============================] - 1s 11ms/step - accuracy: 0.4643 - loss: 2.1240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fced02f2e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./training/ckpt')]\n",
    "with strategy.scope():\n",
    "    multi_worker_model = create_model()\n",
    "multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Save/Restore outside ModelCheckPoint callback__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/model_dir/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving a model\n",
    "# Let `is_chief` be a utility function that inspects the cluster spec and \n",
    "# current task type and returns True if the worker is the chief and False \n",
    "# otherwise.\n",
    "def is_chief():\n",
    "    return True\n",
    "\n",
    "if is_chief():\n",
    "    # This is the model directory will be ideally be a cloud bucket.\n",
    "    path = '/tmp/model_dir'\n",
    "else:\n",
    "    # Save to a path that is unique across workers.\n",
    "    worker_id = 1 \n",
    "    path = '/tmp/model_dir/worker_tmp_' + str(worker_id)\n",
    "\n",
    "multi_worker_model.save(path)\n",
    "\n",
    "# Restoring a checkpoint\n",
    "# On the Chief\n",
    "checkpoint = tf.train.Checkpoint(model=multi_worker_model)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint,\n",
    "    directory=path,\n",
    "    max_to_keep=5\n",
    ")\n",
    "status = checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "# On the Workers\n",
    "# This is the path that the chief saves the model to\n",
    "model_dir_path = '/tmp/model_dir'\n",
    "checkpoint = tf.train.Checkpoint(model=multi_worker_model)\n",
    "manager = tf.train.CheckpointManager(\n",
    "    checkpoint,\n",
    "    directory=path,\n",
    "    max_to_keep=5\n",
    ")\n",
    "latest_checkpoint = tf.train.latest_checkpoint(model_dir_path)\n",
    "status = checkpoint.restore(latest_checkpoint)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
