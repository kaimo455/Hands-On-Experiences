{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import jieba\n",
    "import pathlib\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = '/home/kaimo/.keras/datasets/cmn-eng/cmn.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi.\t嗨。\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #891077 (Martha)\r\n",
      "Hi.\t你好。\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4857568 (musclegirlxyp)\r\n",
      "Run.\t你用跑的。\tCC-BY 2.0 (France) Attribution: tatoeba.org #4008918 (JSakuragi) & #3748344 (egg0073)\r\n",
      "Wait!\t等等！\tCC-BY 2.0 (France) Attribution: tatoeba.org #1744314 (belgavox) & #4970122 (wzhd)\r\n",
      "Wait!\t等一下！\tCC-BY 2.0 (France) Attribution: tatoeba.org #1744314 (belgavox) & #5092613 (mirrorvan)\r\n",
      "Hello!\t你好。\tCC-BY 2.0 (France) Attribution: tatoeba.org #373330 (CK) & #4857568 (musclegirlxyp)\r\n",
      "I try.\t让我来。\tCC-BY 2.0 (France) Attribution: tatoeba.org #20776 (CK) & #5092185 (mirrorvan)\r\n",
      "I won!\t我赢了。\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005192 (CK) & #5102367 (mirrorvan)\r\n",
      "Oh no!\t不会吧。\tCC-BY 2.0 (France) Attribution: tatoeba.org #1299275 (CK) & #5092475 (mirrorvan)\r\n",
      "Cheers!\t乾杯!\tCC-BY 2.0 (France) Attribution: tatoeba.org #487006 (human600) & #765577 (Martha)\r\n"
     ]
    }
   ],
   "source": [
    "!head {path_to_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess sentence\n",
    "def preprocess_sentence(s, is_chinese):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s) # \"\\1\" represents the first matched group, used here for seperating punctuation\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "\n",
    "#     # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "#     s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "\n",
    "    s = s.strip()\n",
    "    \n",
    "    if is_chinese:\n",
    "        # use jieba chinese tokenizer\n",
    "        s = ' '.join(list(jieba.cut(s, cut_all=False)))\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.693 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<start> \\xe4\\xbd\\xa0 \\xe7\\x94\\xa8 \\xe8\\xb7\\x91 \\xe7\\x9a\\x84 \\xe3\\x80\\x82 <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "cn_sentence = u\"你用跑的。\"\n",
    "print(preprocess_sentence(en_sentence, is_chinese=False))\n",
    "print(preprocess_sentence(cn_sentence, is_chinese=True).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, CHINESE]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='utf-8').read().strip().split('\\n')\n",
    "    word_pairs = [(preprocess_sentence(l.split('\\t')[0], is_chinese=False),\n",
    "                   preprocess_sentence(l.split('\\t')[1], is_chinese=True))\n",
    "                  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, cn = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> hi . <end>', '<start> 嗨 。 <end>'),\n",
       " ('<start> hi . <end>', '<start> 你好 。 <end>'),\n",
       " ('<start> run . <end>', '<start> 你 用 跑 的 。 <end>'),\n",
       " ('<start> wait ! <end>', '<start> 等等 ！ <end>'),\n",
       " ('<start> wait ! <end>', '<start> 等 一下 ！ <end>')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(en[:5], cn[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(texts)\n",
    "    # Pad to the longest sequence\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # Creating cleaned input, output pairs\n",
    "    inp_lang, targ_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer = load_dataset(path_to_file, None)\n",
    "# Calculating max_length of the target & input tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17660 4415 17660 4415\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=.2)\n",
    "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "1309 ----> here's\n",
      "27 ----> your\n",
      "448 ----> tea\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "34 ----> 这\n",
      "11 ----> 是\n",
      "7 ----> 你\n",
      "5 ----> 的\n",
      "567 ----> 茶\n",
      "3 ----> 。\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "def convert(lang, tensor):\n",
    "    \"\"\"Convert index to word use lang(tokenizer)\"\"\"\n",
    "    for idx in tensor:\n",
    "        if idx != 0:\n",
    "            print(\"{:d} ----> {:s}\".format(idx, lang.index_word[idx]))\n",
    "\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang_tokenizer, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang_tokenizer, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1 # +1 for padding 0\n",
    "vocab_targ_size = len(targ_lang_tokenizer.word_index) + 1\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 36) (64, 32)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i[0].shape, i[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the encoder and decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True, # Whether to return the last state in addition to the output.\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, initial_state):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=initial_state)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros(shape=(self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 36, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "for i in dataset.take(1):\n",
    "    sample_input_batch, _ = i\n",
    "    sample_output, sample_hidden_state = encoder(sample_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "# Hidden state is the last output of returned sequence tensors\n",
    "# sample_output[:, -1, :] == sample_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial use __Bahdanau attention__ for the encoder.\n",
    "\n",
    "- FC = Full connected (dense) layer\n",
    "- EO = Encoder output (key & value)\n",
    "- H = Hidden state (query)\n",
    "- X = input to the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the pseudo-code:\n",
    "\n",
    "- `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "- `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the 1st axis, since the shape of score is (batch_size, max_length, hidden_size). Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "- `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "- `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "- `merged vector` = concat(embedding output, context vector)\n",
    "- This merged vector is then given to the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # `query` hidden state shape == [BATCH_SIZE, hidden_size], from Decoder's one timestep hidden state\n",
    "        # `query_with_time_axis` shape == [BATCH_SIZE, 1, hidden_size]\n",
    "        # values shape == [BATCH_SIZE, max_len, hidden state], from Encoder's returned sequence hidden states\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # score shape == [BATCH_SIZE, max_length, 1]\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is [BATCH_SIZE, max_length, units]\n",
    "        _FC_H = self.W1(query_with_time_axis) # FC_H of shape [BATCH_SIZE, 1, units]\n",
    "        _FC_EO = self.W2(values) # FC_EO of shape[BATCH_SIZE, max_length, units]\n",
    "        _tanh = tf.nn.tanh(_FC_H + _FC_EO) # _tanh of shape [BATCH_SIZE, max_length, units]\n",
    "        score = self.V(_tanh) # score of shape [BATCH_SIZE, max_length, 1]\n",
    "        \n",
    "        # attension_weights shape == [BACH_SIZE, max_length, 1]\n",
    "        attention_weights = tf.nn.softmax(score, axis=1) # apply on time dimension\n",
    "        \n",
    "        # context_vector shape after sum == [BATCH_SIZE, hidden_size]\n",
    "        context_vector = attention_weights * values # of shape [BATCH_SIZE, max_length, hidden_size]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 36, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(1000) # 1000 is attention fc layer units size\n",
    "        \n",
    "    def call(self, x, dec_hidden, enc_output):\n",
    "        \"\"\"Call function for decoder.\n",
    "        Args:\n",
    "            x: decoder input, of shape [BATCH_SIZE, 1], every time only feed one word\n",
    "            dec_hidden: decoder's hidden state, of shape [BATCH_SIZE, dec_hidden_size]\n",
    "            enc_outputs: encoder's output sequence states, of shape [BATCH_SIZE, max_length, enc_hidden_size]\n",
    "        \"\"\"\n",
    "        # enc_output shape == [BATCH_SIZE, max_length, hidden_size]\n",
    "        # the attention use previous decoder's hidden state as query\n",
    "        context_vector, attention_weights = self.attention(query=dec_hidden, values=enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == [BATCH_SIZE, 1, embedding_dim]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concattenation == [BATCH_SIZE, 1, hidden_size + embedding_dim] (hidden_size of Encoder)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, dec_hidden = self.gru(x)\n",
    "        \n",
    "        # output shape == [BATCH_SIZE * 1, hidden_size]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == [BATCH_SIZE, vocab]\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, dec_hidden, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_targ_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (bach_size, vocab_size) (64, 14055)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (bach_size, vocab_size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"Calculate loss.\n",
    "    Args:\n",
    "        real: ground true, of shape [BATCH_SIZE,]\n",
    "        pred: predictions, of shape [BATCH_SIZE, vocab_size]\n",
    "        \n",
    "    Returns:\n",
    "        A scalar mean loss on batch_size\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_ = loss_object(real, pred) # of shape [BATCH_SIZE,]\n",
    "    # exclude the 0 padding\n",
    "    mask = tf.cast(tf.math.not_equal(real, 0), loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints(Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = pathlib.Path('./training_checkpoints')\n",
    "checkpoint_prefix = checkpoint_dir/'ckpt'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pass the `input` through the `encoder` which return `encoder output` and the `encoder hidden state`.\n",
    "- The `encoder output`, `encoder hidden state` and the `decoder input` (which is the start token) is passed to the `decoder`.\n",
    "- The decoder returns the `predictions` and the `decoder hidden state`.\n",
    "- The `decoder hidden state` is then passed back into the model and the predictions are used to calculate the loss.\n",
    "- Use `teacher forcing` to decide the next input to the decoder.\n",
    "- `Teacher forcing` is the technique where the target word is passed as the next input to the decoder.\n",
    "- The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, initial_enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, initial_state=initial_enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = targ[:, :1] # the <start>\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            # `dec_hidden` is query\n",
    "            # `enc_output` is key & values\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden=dec_hidden, enc_output=enc_output)\n",
    "            \n",
    "            # accumulate loss along time dimension\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 Batch    0 Loss 0.0281\n",
      "Epoch  1 Batch  100 Loss 0.0437\n",
      "Epoch  1 Batch  200 Loss 0.0342\n",
      "Epoch  1 Loss 0.0320\n",
      "Time taken for 1 epoch 96.35619068145752 sec\n",
      "\n",
      "Epoch  2 Batch    0 Loss 0.0304\n",
      "Epoch  2 Batch  100 Loss 0.0194\n",
      "Epoch  2 Batch  200 Loss 0.0292\n",
      "Epoch  2 Loss 0.0276\n",
      "Time taken for 1 epoch 95.85135221481323 sec\n",
      "\n",
      "Epoch  3 Batch    0 Loss 0.0181\n",
      "Epoch  3 Batch  100 Loss 0.0203\n",
      "Epoch  3 Batch  200 Loss 0.0164\n",
      "Epoch  3 Loss 0.0231\n",
      "Time taken for 1 epoch 95.17944645881653 sec\n",
      "\n",
      "Epoch  4 Batch    0 Loss 0.0156\n",
      "Epoch  4 Batch  100 Loss 0.0187\n",
      "Epoch  4 Batch  200 Loss 0.0268\n",
      "Epoch  4 Loss 0.0209\n",
      "Time taken for 1 epoch 95.95512795448303 sec\n",
      "\n",
      "Epoch  5 Batch    0 Loss 0.0162\n",
      "Epoch  5 Batch  100 Loss 0.0217\n",
      "Epoch  5 Batch  200 Loss 0.0264\n",
      "Epoch  5 Loss 0.0220\n",
      "Time taken for 1 epoch 95.03727841377258 sec\n",
      "\n",
      "Epoch  6 Batch    0 Loss 0.0221\n",
      "Epoch  6 Batch  100 Loss 0.0241\n",
      "Epoch  6 Batch  200 Loss 0.0253\n",
      "Epoch  6 Loss 0.0226\n",
      "Time taken for 1 epoch 95.93273735046387 sec\n",
      "\n",
      "Epoch  7 Batch    0 Loss 0.0181\n",
      "Epoch  7 Batch  100 Loss 0.0168\n",
      "Epoch  7 Batch  200 Loss 0.0171\n",
      "Epoch  7 Loss 0.0195\n",
      "Time taken for 1 epoch 95.09082961082458 sec\n",
      "\n",
      "Epoch  8 Batch    0 Loss 0.0171\n",
      "Epoch  8 Batch  100 Loss 0.0152\n",
      "Epoch  8 Batch  200 Loss 0.0125\n",
      "Epoch  8 Loss 0.0171\n",
      "Time taken for 1 epoch 95.61004590988159 sec\n",
      "\n",
      "Epoch  9 Batch    0 Loss 0.0140\n",
      "Epoch  9 Batch  100 Loss 0.0174\n",
      "Epoch  9 Batch  200 Loss 0.0201\n",
      "Epoch  9 Loss 0.0159\n",
      "Time taken for 1 epoch 95.3895492553711 sec\n",
      "\n",
      "Epoch 10 Batch    0 Loss 0.0205\n",
      "Epoch 10 Batch  100 Loss 0.0134\n",
      "Epoch 10 Batch  200 Loss 0.0184\n",
      "Epoch 10 Loss 0.0145\n",
      "Time taken for 1 epoch 95.70176792144775 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    initial_enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch_idx, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, initial_enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {:2d} Batch {:4d} Loss {:.4f}'.format(epoch+1, batch_idx, batch_loss.numpy()))\n",
    "            \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=str(checkpoint_prefix))\n",
    "        \n",
    "    print('Epoch {:2d} Loss {:.4f}'.format(epoch+1, total_loss/steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence, is_chinese=False)\n",
    "    \n",
    "    inputs = inp_lang_tokenizer.texts_to_sequences([sentence])[0]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    initial_hidden_state = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden_state = encoder(inputs, initial_hidden_state)\n",
    "    \n",
    "    dec_hidden_state = enc_hidden_state\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_state, attention_weights = decoder(dec_input, dec_hidden_state, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        # before `reshape`: of shape [BATCH_SIZE(1), max_length_inp, 1]\n",
    "        # after `reshape`: of shape [max_length_inp,]\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        prediction_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += targ_lang_tokenizer.index_word[prediction_id] + ' '\n",
    "        \n",
    "        # reach the end of sentence\n",
    "        if targ_lang_tokenizer.index_word[prediction_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        else:\n",
    "            dec_input = tf.expand_dims([prediction_id], 0)\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print('Input: {:s}'.format(sentence))\n",
    "    print('Predicted translation: {:s}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, sentence, attention_plot = evaluate('how are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i miss you <end>\n",
      "Predicted translation: 我 如此 见 。 <end> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 22914 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 27492 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 35265 missing from current font.\n",
      "  font.set_text(s, 0.0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 22914 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 27492 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n",
      "/opt/conda/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 35265 missing from current font.\n",
      "  font.set_text(s, 0, flags=flags)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('I miss you')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f436d201950>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the lastest checkpoint in checkpoint_dir.\n",
    "# remind that `checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)`\n",
    "# after restoring, the optimzier, encoder and decoder are update to latest checkpoint values.\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
