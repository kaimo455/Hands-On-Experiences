{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import pathlib\n",
    "import unicodedata\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(fname='spa-eng.zip',\n",
    "                                      origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "                                      extract=True)\n",
    "path_to_zip = pathlib.Path(path_to_zip)\n",
    "\n",
    "path_to_file = path_to_zip.parents[0]/'spa-eng/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tVe.\r\n",
      "Go.\tVete.\r\n",
      "Go.\tVaya.\r\n",
      "Go.\tVáyase.\r\n",
      "Hi.\tHola.\r\n",
      "Run!\t¡Corre!\r\n",
      "Run.\tCorred.\r\n",
      "Who?\t¿Quién?\r\n",
      "Fire!\t¡Fuego!\r\n",
      "Fire!\t¡Incendio!\r\n"
     ]
    }
   ],
   "source": [
    "!head {path_to_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess sentence\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s) # \"\\1\" represents the first matched group, used here for seperating punctuation\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
    "\n",
    "    s = s.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    s = '<start> ' + s + ' <end>'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book ? <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='utf-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(s) for s in l.split('\\t')] \n",
    "                  for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "en, sp = create_dataset(path_to_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<start> go . <end>', '<start> ve . <end>'),\n",
       " ('<start> go . <end>', '<start> vete . <end>'),\n",
       " ('<start> go . <end>', '<start> vaya . <end>'),\n",
       " ('<start> go . <end>', '<start> vayase . <end>'),\n",
       " ('<start> hi . <end>', '<start> hola . <end>')]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(en[:5], sp[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(texts)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(texts)\n",
    "    # Pad to the longest sequence\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    # Creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenzier = load_dataset(path_to_file, None)\n",
    "# Calculating max_length of the target & input tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95171 23793 95171 23793\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=.2)\n",
    "print(len(input_tensor_train), len(input_tensor_val), len(target_tensor_train), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    \"\"\"Convert index to word use lang(tokenizer)\"\"\"\n",
    "    for idx in tensor:\n",
    "        if idx != 0:\n",
    "            print(\"{:d} ----> {:s}\".format(idx, lang.index_word[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "12 ----> ¿\n",
      "79 ----> quien\n",
      "14 ----> es\n",
      "10 ----> la\n",
      "233 ----> persona\n",
      "34 ----> mas\n",
      "170 ----> sabia\n",
      "4 ----> que\n",
      "781 ----> conoces\n",
      "11 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "80 ----> who\n",
      "11 ----> is\n",
      "5 ----> the\n",
      "10433 ----> wisest\n",
      "279 ----> person\n",
      "7 ----> you\n",
      "45 ----> know\n",
      "10 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang_tokenizer, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang_tokenzier, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang_tokenizer.word_index) + 1 # +1 for padding 0\n",
    "vocab_targ_size = len(targ_lang_tokenzier.word_index) + 1\n",
    "\n",
    "dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train))\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 53) (64, 51)\n"
     ]
    }
   ],
   "source": [
    "for i in dataset.take(1):\n",
    "    print(i[0].shape, i[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the encoder and decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.enc_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True, # Whether to return the last state in addition to the output.\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        \n",
    "    def call(self, x, initial_state):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=initial_state)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros(shape=(self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 53, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "for i in dataset.take(1):\n",
    "    sample_input_batch, _ = i\n",
    "    sample_output, sample_hidden_state = encoder(sample_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n",
    "\n",
    "# Hidden state is the last output of returned sequence tensors\n",
    "# sample_output[:, -1, :] == sample_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial use __Bahdanau attention__ for the encoder.\n",
    "\n",
    "- FC = Full connected (dense) layer\n",
    "- EO = Encoder output (key & value)\n",
    "- H = Hidden state (query)\n",
    "- X = input to the decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the pseudo-code:\n",
    "\n",
    "- `score = FC(tanh(FC(EO) + FC(H)))`\n",
    "- `attention weights = softmax(score, axis = 1)`. Softmax by default is applied on the last axis but here we want to apply it on the 1st axis, since the shape of score is (batch_size, max_length, hidden_size). Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\n",
    "- `context vector = sum(attention weights * EO, axis = 1)`. Same reason as above for choosing axis as 1.\n",
    "- `embedding output` = The input to the decoder X is passed through an embedding layer.\n",
    "- `merged vector` = concat(embedding output, context vector)\n",
    "- This merged vector is then given to the GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # `query` hidden state shape == [BATCH_SIZE, hidden_size], from Decoder's one timestep hidden state\n",
    "        # `query_with_time_axis` shape == [BATCH_SIZE, 1, hidden_size]\n",
    "        # values shape == [BATCH_SIZE, max_len, hidden state], from Encoder's returned sequence hidden states\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # score shape == [BATCH_SIZE, max_length, 1]\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is [BATCH_SIZE, max_length, units]\n",
    "        _FC_H = self.W1(query_with_time_axis) # FC_H of shape [BATCH_SIZE, 1, units]\n",
    "        _FC_EO = self.W2(values) # FC_EO of shape[BATCH_SIZE, max_length, units]\n",
    "        _tanh = tf.nn.tanh(_FC_H + _FC_EO) # _tanh of shape [BATCH_SIZE, max_length, units]\n",
    "        score = self.V(_tanh) # score of shape [BATCH_SIZE, max_length, 1]\n",
    "        \n",
    "        # attension_weights shape == [BACH_SIZE, max_length, 1]\n",
    "        attention_weights = tf.nn.softmax(score, axis=1) # apply on time dimension\n",
    "        \n",
    "        # context_vector shape after sum == [BATCH_SIZE, hidden_size]\n",
    "        context_vector = attention_weights * values # of shape [BATCH_SIZE, max_length, hidden_size]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 53, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, **kwargs):\n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(\n",
    "            self.dec_units,\n",
    "            return_sequences=True,\n",
    "            return_state=True,\n",
    "            recurrent_initializer='glorot_uniform'\n",
    "        )\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(500) # 500 is attention fc layer units size\n",
    "        \n",
    "    def call(self, x, dec_hidden, enc_output):\n",
    "        \"\"\"Call function for decoder.\n",
    "        Args:\n",
    "            x: decoder input, of shape [BATCH_SIZE, 1], every time only feed one word\n",
    "            dec_hidden: decoder's hidden state, of shape [BATCH_SIZE, dec_hidden_size]\n",
    "            enc_outputs: encoder's output sequence states, of shape [BATCH_SIZE, max_length, enc_hidden_size]\n",
    "        \"\"\"\n",
    "        # enc_output shape == [BATCH_SIZE, max_length, hidden_size]\n",
    "        # the attention use previous decoder's hidden state as query\n",
    "        context_vector, attention_weights = self.attention(query=dec_hidden, values=enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == [BATCH_SIZE, 1, embedding_dim]\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concattenation == [BATCH_SIZE, 1, hidden_size + embedding_dim] (hidden_size of Encoder)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, axis=1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, dec_hidden = self.gru(x)\n",
    "        \n",
    "        # output shape == [BATCH_SIZE * 1, hidden_size]\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == [BATCH_SIZE, vocab]\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, dec_hidden, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_targ_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (bach_size, vocab_size) (64, 12934)\n"
     ]
    }
   ],
   "source": [
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print('Decoder output shape: (bach_size, vocab_size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    \"\"\"Calculate loss.\n",
    "    Args:\n",
    "        real: ground true, of shape [BATCH_SIZE,]\n",
    "        pred: predictions, of shape [BATCH_SIZE, vocab_size]\n",
    "        \n",
    "    Returns:\n",
    "        A scalar mean loss on batch_size\n",
    "    \"\"\"\n",
    "    \n",
    "    loss_ = loss_object(real, pred) # of shape [BATCH_SIZE,]\n",
    "    # exclude the 0 padding\n",
    "    mask = tf.cast(tf.math.not_equal(real, 0), loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpoints(Object-based saving)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = pathlib.Path('./training_checkpoints')\n",
    "checkpoint_prefix = checkpoint_dir/'ckpt'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pass the `input` through the `encoder` which return `encoder output` and the `encoder hidden state`.\n",
    "- The `encoder output`, `encoder hidden state` and the `decoder input` (which is the start token) is passed to the `decoder`.\n",
    "- The decoder returns the `predictions` and the `decoder hidden state`.\n",
    "- The `decoder hidden state` is then passed back into the model and the predictions are used to calculate the loss.\n",
    "- Use `teacher forcing` to decide the next input to the decoder.\n",
    "- `Teacher forcing` is the technique where the target word is passed as the next input to the decoder.\n",
    "- The final step is to calculate the gradients and apply it to the optimizer and backpropagate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, initial_enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, initial_state=initial_enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = targ[:, :1] # the <start>\n",
    "        \n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            # `dec_hidden` is query\n",
    "            # `enc_output` is key & values\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden=dec_hidden, enc_output=enc_output)\n",
    "            \n",
    "            # accumulate loss along time dimension\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1, EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    initial_enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch_idx, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, initial_enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Epoch {:2d} Batch {:4d} Loss {:.4f}'.format(epoch+1, batch_idx, batch_loss.numpy()))\n",
    "            \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix=str(checkpoint_prefix))\n",
    "        \n",
    "    print('Epoch {:2d} Loss {:.4f}'.format(epoch+1, total_loss/steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    initial_hidden_state = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden_state = encoder(inputs, initial_hidden_state)\n",
    "    \n",
    "    dec_hidden_state = enc_hidden_state\n",
    "    dec_input = tf.expand_dims([targ_lang_tokenzier.word_index['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden_state, attention_weights = decoder(dec_input, dec_hidden_state, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        # before `reshape`: of shape [BATCH_SIZE(1), max_length_inp, 1]\n",
    "        # after `reshape`: of shape [max_length_inp,]\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        prediction_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += targ_lang_tokenzier.index_word[prediction_id] + ' '\n",
    "        \n",
    "        # reach the end of sentence\n",
    "        if targ_lang_tokenzier.index_word[prediction_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        else:\n",
    "            dec_input = tf.expand_dims([prediction_id], 0)\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print('Input: {:s}'.format(sentence))\n",
    "    print('Predicted translation: {:s}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> ¿ todavia estan en casa ? <end>\n",
      "Predicted translation: are you still at home ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f436d201950>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the lastest checkpoint in checkpoint_dir.\n",
    "# remind that `checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)`\n",
    "# after restoring, the optimzier, encoder and decoder are update to latest checkpoint values.\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hace mucho frio aqui . <end>\n",
      "Predicted translation: it s very cold here . <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now we can use restore encoder, decoder to translate\n",
    "translate(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
