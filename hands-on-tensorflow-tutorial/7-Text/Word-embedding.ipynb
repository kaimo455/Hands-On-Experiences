{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset imdb_reviews (80.23 MiB) to /home/kaimo/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0...\u001b[0m\n",
      "Shuffling and writing examples to /home/kaimo/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete3B2EE8/imdb_reviews-train.tfrecord\n",
      "Shuffling and writing examples to /home/kaimo/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete3B2EE8/imdb_reviews-test.tfrecord\n",
      "Shuffling and writing examples to /home/kaimo/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0.incomplete3B2EE8/imdb_reviews-unsupervised.tfrecord\n",
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /home/kaimo/tensorflow_datasets/imdb_reviews/subwords8k/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k',\n",
    "                                          split=[tfds.Split.TRAIN, tfds.Split.TEST],\n",
    "                                          with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the encoder (`tfds.features.text.SubwordTextEncoder`), and have a quick look at the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the_',\n",
       " ', ',\n",
       " '. ',\n",
       " 'a_',\n",
       " 'and_',\n",
       " 'of_',\n",
       " 'to_',\n",
       " 's_',\n",
       " 'is_',\n",
       " 'br',\n",
       " 'in_',\n",
       " 'I_',\n",
       " 'that_',\n",
       " 'this_',\n",
       " 'it_',\n",
       " ' /><',\n",
       " ' />',\n",
       " 'was_',\n",
       " 'The_',\n",
       " 'as_']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = info.features['text'].encoder\n",
    "encoder.subwords[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((None, None), (None,)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.padded_batch(10, ([None,], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10, ([None], []))\n",
    "test_batches = test_data.padded_batch(10, ([None], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simgle model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=encoder.vocab_size, output_dim=embedding_size),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2500/2500 [==============================] - 16s 7ms/step - loss: 0.5086 - accuracy: 0.6987 - val_loss: 0.3512 - val_accuracy: 0.8370\n",
      "Epoch 2/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.2870 - accuracy: 0.8820 - val_loss: 0.3107 - val_accuracy: 0.8774\n",
      "Epoch 3/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.2312 - accuracy: 0.9079 - val_loss: 0.3194 - val_accuracy: 0.8820\n",
      "Epoch 4/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1997 - accuracy: 0.9228 - val_loss: 0.3334 - val_accuracy: 0.8808\n",
      "Epoch 5/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1750 - accuracy: 0.9340 - val_loss: 0.3364 - val_accuracy: 0.8753\n",
      "Epoch 6/10\n",
      "2500/2500 [==============================] - 16s 7ms/step - loss: 0.1597 - accuracy: 0.9413 - val_loss: 0.3625 - val_accuracy: 0.8604\n",
      "Epoch 7/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1447 - accuracy: 0.9469 - val_loss: 0.3848 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1313 - accuracy: 0.9516 - val_loss: 0.4093 - val_accuracy: 0.8669\n",
      "Epoch 9/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1193 - accuracy: 0.9568 - val_loss: 0.4470 - val_accuracy: 0.8596\n",
      "Epoch 10/10\n",
      "2500/2500 [==============================] - 16s 6ms/step - loss: 0.1133 - accuracy: 0.9596 - val_loss: 0.4767 - val_accuracy: 0.8547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc2091a9450>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_batches, epochs=10, validation_data=test_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8185, 16)\n"
     ]
    }
   ],
   "source": [
    "embed = model.layers[0]\n",
    "weights = embed.get_weights()[0]\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save learned embedding vectors and vocab to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, vec_file = tempfile.mkstemp(suffix='.tsv')\n",
    "_, meta_file = tempfile.mkstemp(suffix='.tsv')\n",
    "\n",
    "out_vec = io.open(vec_file, 'w', encoding='utf-8')\n",
    "out_m = io.open(meta_file, 'w', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, word in enumerate(encoder.subwords):\n",
    "    vec = weights[idx + 1] # skip 0, it's padding\n",
    "    out_m.write(word + '\\n')\n",
    "    out_vec.write('\\t'.join([str(x) for x in vec]) + '\\n')\n",
    "out_vec.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.033357132\t-0.18019158\t0.019915733\t0.05698549\t0.095509514\t0.028049178\t0.07198314\t0.044673074\t0.08024245\t-0.09789379\t-0.038111404\t0.06030338\t0.08713607\t0.045528017\t0.0650031\t0.10778238\r\n",
      "-0.011729498\t-0.08036991\t0.0344378\t0.037724752\t0.043161742\t-0.010260238\t0.058903255\t0.0031632227\t0.0750861\t0.0062241\t-0.023241108\t0.08388362\t-0.0099297315\t-0.025998624\t-0.036804646\t0.022164281\r\n",
      "0.021704625\t-0.026616946\t0.08833752\t-0.06613831\t-0.06283467\t-0.026924789\t-0.024841247\t-0.029082617\t-0.030629328\t0.0181491\t0.049597535\t0.006083052\t-0.011908604\t-0.04640039\t-0.102714196\t-0.05951492\r\n",
      "-0.035312433\t-0.09898356\t0.07880238\t-0.0023776633\t0.0066279373\t-0.02269177\t0.024111446\t0.009804823\t0.07577569\t0.0050103036\t-0.013129178\t0.012743426\t0.028365336\t-0.015440008\t0.025633333\t0.09226063\r\n",
      "-0.10275722\t-0.13000952\t0.016330292\t0.1523128\t0.065438606\t0.0284161\t0.15065762\t0.109051965\t0.093990445\t-0.15007022\t-0.018336393\t0.08064745\t0.062052786\t0.03342415\t0.034527965\t0.09858491\r\n",
      "0.106824055\t0.020318193\t0.14062102\t-0.054308847\t-0.08716202\t-0.0765414\t0.01805198\t-0.07420554\t-0.032212913\t0.030549848\t0.039323147\t-0.036935035\t-0.07860425\t-0.04473832\t-0.14126457\t-0.066739194\r\n",
      "0.057977807\t-0.099353746\t0.09746827\t-0.00616311\t-0.037402324\t-0.012967339\t0.02134346\t-0.08736636\t0.046765912\t-0.026016962\t0.031443242\t0.07567518\t-0.058534674\t-0.028252207\t-0.09453312\t0.017545378\r\n",
      "-0.038885705\t-0.06346731\t0.0404779\t0.02966187\t0.056676883\t0.028978815\t0.079364076\t0.07314646\t0.08803906\t-0.1064891\t-0.045151986\t0.027224416\t-0.037164435\t0.0014318121\t-0.03446229\t0.07132347\r\n",
      "-0.019252984\t-0.10799811\t-0.014700616\t0.06692086\t0.02488426\t-0.004772997\t0.0573014\t0.07115601\t0.09713549\t-0.042778324\t-0.021285133\t0.043136053\t0.054571535\t0.012644321\t0.02045921\t0.11222932\r\n",
      "0.02612042\t-0.020798905\t0.06943247\t-0.03912074\t-0.019713305\t-0.033646803\t0.0075232536\t-0.11298787\t0.056062974\t-0.03353418\t0.008156223\t-0.050975535\t-0.08986417\t-0.091523215\t-0.026469765\t-0.056800995\r\n"
     ]
    }
   ],
   "source": [
    "!head {vec_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_\r\n",
      ", \r\n",
      ". \r\n",
      "a_\r\n",
      "and_\r\n",
      "of_\r\n",
      "to_\r\n",
      "s_\r\n",
      "is_\r\n",
      "br\r\n"
     ]
    }
   ],
   "source": [
    "!head {meta_file}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
